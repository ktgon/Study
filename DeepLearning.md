### 진행 방향 
- 1권: 순전파, 역전파, CNN  
- 2권: Embedding, RNN
- Attention
- Transformer

### 2025-02-04 

#### 역사
1940년 중반부터 신경망이 발전되기 시작  
1950년부터 인공지능에 대해 본격적으로 연구됨 -> 엘런튜링  
튜링테스트: AI에 대한 정의 (칸막이 뒤 컴퓨터와 대화시 기계여부를 판단), 2010년대까지 사용(현재는 사람보다 더 사람같으므로 미사용)  
튜링머신: 컴퓨터에게 명령을 내리는 프로세스를 정의  (CSS/HTML은 튜링머신에 따른 프로세스가 성립되지 않으므로 프로그래밍 언어가 아니다.)  

1950년대 말 다트머스 대학 컨퍼런스에서 인공지능 사용.  
별개로 뉴런을 알게 되었고 이를 기본으로 신경망이라는 컨셉이 나옴.  

컴퓨터 과학 <-> 신경 과학  
마빈 민스키가 반대 <-> 1957년도에 프랑크 로젠블라트에 의해 퍼셉트론이 고안됨.  
(프로드세넌, 민스키가 센서의 기초를 닦음.)

전문가 이론 <-> 1차 AI 겨울 (30년)  
퍼지 이론 <->  XOR문제를 해결할 방법은 나왔으나 실용성이 없었음.  
유전자 알고리즘 <-> 86년 제프리힌턴이 이를 실용성있는 방법을 찾음.  

2차 AI 겨울 -> 통계의 범위를 못벗어남. 

2006년에 제프리힌턴이 기울기 소실문제를 알아냄.  -> DeepLearning 시작됨  

별도로 Convolution, Recursive 기법이 발달함.  
Convolution은 수학의 합성곱을 사용하여 효율적 영상을 처리  
Recursive 출력을 다시 입력으로 또 다른 입력을 받으면 효과가 있음  

DeepLearning + Convolution = CNN  
DeepLearning + Recursive = RNN  

2010년 페이페이리 ->  워드넷과 같은 이미지넷을 만듬.  
워드넷은 86년부터 만들어진 ontology -> 사회/윤리를 컴퓨터에서 처리할 수 있도록 체계화한 모델  

2012년도의 1등은 월등한 점수를 기록함.  (제프리힌턴, 알렉스 크리제프스키, 알리야 수츠케버) -> AlexNet 사용  
AlexNet 팀을 Google에서 사감.  

#### 이론  
의사결정나무 -> 분류를 한다면 Gini Coefficient를 본다. -> 지니계수가 가장 큰 부분을 찾는다. -> 이것을 tree 구조로 표현한다. => 조건을 데이터에서 찾는 것이다. (Machine Learning)  
로젠블라트 -> 퍼셉트론 

#### 기타  
가중합에 대한 activation 함수 적용을 한 경우 XOR 처리가 안됨.  
위의 연산을 하나 더 추가하여 가중합을 구한 후 activation 함수를 적용하는 방식으로 하면 XOR이 분류가 되는 걸 발견함.  
경우에 따라 레이어를 만들어 추가하는 방식으로 모델러가 모델링을 한 후  
임의의 가중값을 부여한 후 이걸 반복하면서 보정해 나가는 것이 Machine Learning 이다.  
하지만 이렇게 하려면 하드웨어 성능이 필요하므로 시간이 오래 걸렸다.  
모델은 사람이 조절한다. 단지 가중치에 대한 부분은 하드웨어가 처리한다.  

레이어구성 -> 학습 -> 평가 를 반복한다. 
입력데이터가 바뀌면 모델을 변경해줘야한다. 
모든 것을 풀 수 있는 모델은 없다.  
layer의 뉴런 개수가 입력개수를 따라가는 건 아니다.  

뉴런 개수가 많고 layer가 많을 수록 성능이 잘 나온다. -> 자원 소모  

처음엔 순전파로만 해를 구했다.  
파라미터 값을 처음엔 무작위로 준다. 이 후 보정을 한다.  
80년대 컴퓨터로는 이를 해결할 방법이 없었다.  
어느 부분이 많은 리소스를 쓰는지를 찾았다.  
(아래 부분이다.)
결정을 작위로 할 수 없으므로 최적화 알고리즘을 쓴다.  
평균제곱오차를 보정하는 값을 만들어 준다.  
오차 곡선(?)의 기울기를 줄일 수 있는 방향으로 보정한다.  
미분을 계속 하면서 보정해간다.  
0.000008 간격으로 찾는다. (더 이상 작으면 계산 불가...)  
파라미터가 많으므로 이를 처리할 방법이 없었다.  

제프리힌턴이 역전파 방식으로 하게 되면 미분이 줄어드는 방식을 사용해 계산량을 줄였다.  
순전파와 역전파가 별차이가 없음을 발견했음.  

잘되다가 어느 순간부터는 보정이 안되는 현상이 나옴.  -> 이걸 찾는데 20년이 걸림.  
시그모이드 이슈이다.  
시그모이드 함수의 값은 미분하면 0.3에서 올라가지를 않게 된다. (기울기 소실문제)  
중간 activation함수를 ReLU 함수로 교체, 마지막은 시그모이드 함수를 사용하는 방식으로 해결  

CNN 자른 후 변조를 시켜 학습을 하는 방법이다.  
tensor는 행렬이 3개가 엮은 것을 말한다.  

분류와 예측  
분류와 예측은 둘 다 understanding 이고  
생성형은 generation이다.  

RNN - 입력으로 나온 출력을 다시 입력으로 넣고 다른 곳에서 또 입력을 넣고 하는 방식으로 처리한다.  (분석, 생성 다 가능하다.)  
입력은 encoding 이고 출력은 decoding 이다.  

tension - 좀 더 중용한 파라미터 위주로 처리  
비지도 학습 - 답지가 없는 것, 벡터값의 거리가 멀어지면 분류하여 처리  
지도 학습 - 답지가 있는 것  
강화 학습 - 스스로의 행동에 대한 보상을 받으며 학습 진행 (사전 정의된 규칙은 필요)  

Embedding 
Bag of Word, Corpus  
단어를 벡터값으로 바꿔서 단어간의 거리(친밀도)를 구할 수 있다. 
Bag of Word 이용해서 예를 들어 i love you 가 있다면 [100], [010], [001] 과 같이 벡터로 표현한다.  
하지만 이렇게 하게 되면 아주 큰 Bag Of Word의 경우 0이 너무 많으므로(희소벡터) 이를 줄이는 방법으로 
수식을 (차원축소기법) 이용하여 밀집벡터를 만들어쓰기도 한다.

Bag Of Word를 백과사전의 스포츠관련된 것을 중심으로 묶었다면  
여기 있는 단어와 유사한 단어가 많다면(TF:term frequency) 해당 글이 스포츠관련된 것으로 판단한다.  
빈도는 많지만 의미없는 단어는 (a, an ...) 점수를 깎는다. (IDF: inverse document frequency)  
Corpus 전체에서 빈도가 많을 수록 점수를 깎게된다.  

이제는 분포가설을 통해 벡터를 구한다.  
분석을 하려는 단어 앞뒤의 단어를 통해 단어의 의미를 판단한다.  
이전부터 있던 것을 활용한 것이다.  

=> 그 다음은 기억이 나지 않음  

LLM은 언어별를 처리하는 방식이 다르다.  



